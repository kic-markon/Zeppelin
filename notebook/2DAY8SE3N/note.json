{
  "paragraphs": [
    {
      "text": "%md\nThe following example is taken from:\nAdvanced Analytics with Spark, 2nd Edition\nby Sandy Ryza, Uri Laserson, Sean Owen and Josh Wills\n(c) O\u0027Reilly, 2017\n\nInitial steps for creating a Random Forest Classifier for the \"Forest Cover\" problem.",
      "user": "anonymous",
      "dateUpdated": "Apr 14, 2018 10:34:33 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eThe following example is taken from:\u003cbr/\u003eAdvanced Analytics with Spark, 2nd Edition\u003cbr/\u003eby Sandy Ryza, Uri Laserson, Sean Owen and Josh Wills\u003cbr/\u003e(c) O\u0026rsquo;Reilly, 2017\u003c/p\u003e\n\u003cp\u003eInitial steps for creating a Random Forest Classifier for the \u0026ldquo;Forest Cover\u0026rdquo; problem.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1523712828873_37440770",
      "id": "20180414-223348_355150027",
      "dateCreated": "Apr 14, 2018 10:33:48 PM",
      "dateStarted": "Apr 14, 2018 10:34:33 PM",
      "dateFinished": "Apr 14, 2018 10:34:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import org.apache.spark.ml.{PipelineModel, Pipeline}\nimport org.apache.spark.ml.classification.{DecisionTreeClassifier,\n  RandomForestClassifier, RandomForestClassificationModel}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nimport org.apache.spark.sql.functions._\nimport scala.util.Random\n",
      "user": "anonymous",
      "dateUpdated": "Mar 22, 2018 10:13:18 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.ml.{PipelineModel, Pipeline}\nimport org.apache.spark.ml.classification.{DecisionTreeClassifier, RandomForestClassifier, RandomForestClassificationModel}\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.feature.{VectorAssembler, VectorIndexer}\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nimport org.apache.spark.sql.functions._\nimport scala.util.Random\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1521208102341_-269572481",
      "id": "20180316-224822_2095860959",
      "dateCreated": "Mar 16, 2018 10:48:22 PM",
      "dateStarted": "Mar 22, 2018 10:13:18 PM",
      "dateFinished": "Mar 22, 2018 10:13:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    import spark.implicits._\n\n    val dataWithoutHeader \u003d spark.read.\n      option(\"inferSchema\", true).\n      option(\"header\", false).\n      csv(\"/Users/markon/ITU/covtype.data\")\n\n    val colNames \u003d Seq(\n        \"Elevation\", \"Aspect\", \"Slope\",\n        \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\",\n        \"Horizontal_Distance_To_Roadways\",\n        \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\",\n        \"Horizontal_Distance_To_Fire_Points\"\n      ) ++ (\n        (0 until 4).map(i \u003d\u003e s\"Wilderness_Area_$i\")\n      ) ++ (\n        (0 until 40).map(i \u003d\u003e s\"Soil_Type_$i\")\n      ) ++ Seq(\"Cover_Type\")\n\n    val data \u003d dataWithoutHeader.toDF(colNames:_*).\n      withColumn(\"Cover_Type\", $\"Cover_Type\".cast(\"double\"))\n\n    data.show()\n    data.head\n\n    // Split into 90% train (+ CV), 10% test\n    val Array(trainData, testData) \u003d data.randomSplit(Array(0.9, 0.1))\n",
      "user": "anonymous",
      "dateUpdated": "Mar 22, 2018 10:16:34 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import spark.implicits._\ndataWithoutHeader: org.apache.spark.sql.DataFrame \u003d [_c0: int, _c1: int ... 53 more fields]\ncolNames: Seq[String] \u003d List(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, Soil_...data: org.apache.spark.sql.DataFrame \u003d [Elevation: int, Aspect: int ... 53 more fields]\n+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+----------+\n|Elevation|Aspect|Slope|Horizontal_Distance_To_Hydrology|Vertical_Distance_To_Hydrology|Horizontal_Distance_To_Roadways|Hillshade_9am|Hillshade_Noon|Hillshade_3pm|Horizontal_Distance_To_Fire_Points|Wilderness_Area_0|Wilderness_Area_1|Wilderness_Area_2|Wilderness_Area_3|Soil_Type_0|Soil_Type_1|Soil_Type_2|Soil_Type_3|Soil_Type_4|Soil_Type_5|Soil_Type_6|Soil_Type_7|Soil_Type_8|Soil_Type_9|Soil_Type_10|Soil_Type_11|Soil_Type_12|Soil_Type_13|Soil_Type_14|Soil_Type_15|Soil_Type_16|Soil_Type_17|Soil_Type_18|Soil_Type_19|Soil_Type_20|Soil_Type_21|Soil_Type_22|Soil_Type_23|Soil_Type_24|Soil_Type_25|Soil_Type_26|Soil_Type_27|Soil_Type_28|Soil_Type_29|Soil_Type_30|Soil_Type_31|Soil_Type_32|Soil_Type_33|Soil_Type_34|Soil_Type_35|Soil_Type_36|Soil_Type_37|Soil_Type_38|Soil_Type_39|Cover_Type|\n+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+----------+\n|     2596|    51|    3|                             258|                             0|                            510|          221|           232|          148|                              6279|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2590|    56|    2|                             212|                            -6|                            390|          220|           235|          151|                              6225|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2804|   139|    9|                             268|                            65|                           3180|          234|           238|          135|                              6121|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       2.0|\n|     2785|   155|   18|                             242|                           118|                           3090|          238|           238|          122|                              6211|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       2.0|\n|     2595|    45|    2|                             153|                            -1|                            391|          220|           234|          150|                              6172|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2579|   132|    6|                             300|                           -15|                             67|          230|           237|          140|                              6031|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       2.0|\n|     2606|    45|    7|                             270|                             5|                            633|          222|           225|          138|                              6256|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2605|    49|    4|                             234|                             7|                            573|          222|           230|          144|                              6228|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2617|    45|    9|                             240|                            56|                            666|          223|           221|          133|                              6244|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2612|    59|   10|                             247|                            11|                            636|          228|           219|          124|                              6230|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2612|   201|    4|                             180|                            51|                            735|          218|           243|          161|                              6222|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2886|   151|   11|                             371|                            26|                           5253|          234|           240|          136|                              4051|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       2.0|\n|     2742|   134|   22|                             150|                            69|                           3215|          248|           224|           92|                              6091|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       2.0|\n|     2609|   214|    7|                             150|                            46|                            771|          213|           247|          170|                              6211|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2503|   157|    4|                              67|                             4|                            674|          224|           240|          151|                              5600|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2495|    51|    7|                              42|                             2|                            752|          224|           225|          137|                              5576|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2610|   259|    1|                             120|                            -1|                            607|          216|           239|          161|                              6096|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2517|    72|    7|                              85|                             6|                            595|          228|           227|          133|                              5607|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2504|     0|    4|                              95|                             5|                            691|          214|           232|          156|                              5572|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n|     2503|    38|    5|                              85|                            10|                            741|          220|           228|          144|                              5555|                1|                0|                0|                0|          0|          0|          0|          0|          0|          0|          0|          0|          0|          0|           0|           0|           0|           0|           0|           0|           0|           1|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|           0|       5.0|\n+---------+------+-----+--------------------------------+------------------------------+-------------------------------+-------------+--------------+-------------+----------------------------------+-----------------+-----------------+-----------------+-----------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+------------+----------+\nonly showing top 20 rows\n\nres5: org.apache.spark.sql.Row \u003d [2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5.0]\ntrainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [Elevation: int, Aspect: int ... 53 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [Elevation: int, Aspect: int ... 53 more fields]\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1521208137017_-346275429",
      "id": "20180316-224857_812128568",
      "dateCreated": "Mar 16, 2018 10:48:57 PM",
      "dateStarted": "Mar 22, 2018 10:16:34 PM",
      "dateFinished": "Mar 22, 2018 10:16:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "     val inputCols \u003d trainData.columns.filter(_ !\u003d \"Cover_Type\")\n    val assembler \u003d new VectorAssembler().\n      setInputCols(inputCols).\n      setOutputCol(\"featureVector\")\n\n    val assembledTrainData \u003d assembler.transform(trainData)\n    assembledTrainData.select(\"featureVector\").show(truncate \u003d false)\n\n    val classifier \u003d new DecisionTreeClassifier().\n      setSeed(Random.nextLong()).\n      setLabelCol(\"Cover_Type\").\n      setFeaturesCol(\"featureVector\").\n      setPredictionCol(\"prediction\")\n\n    val model \u003d classifier.fit(assembledTrainData)\n    println(model.toDebugString)\n\n    model.featureImportances.toArray.zip(inputCols).\n      sorted.reverse.foreach(println)\n\n    val predictions \u003d model.transform(assembledTrainData)\n\n    predictions.select(\"Cover_Type\", \"prediction\", \"probability\").\n      show(truncate \u003d false)\n\n    val evaluator \u003d new MulticlassClassificationEvaluator().\n      setLabelCol(\"Cover_Type\").\n      setPredictionCol(\"prediction\")\n\n    val accuracy \u003d evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n    val f1 \u003d evaluator.setMetricName(\"f1\").evaluate(predictions)\n    println(accuracy)\n    println(f1)\n\n    val predictionRDD \u003d predictions.\n      select(\"prediction\", \"Cover_Type\").\n      as[(Double,Double)].rdd\n    val multiclassMetrics \u003d new MulticlassMetrics(predictionRDD)\n    println(multiclassMetrics.confusionMatrix)\n\n    val confusionMatrix \u003d predictions.\n      groupBy(\"Cover_Type\").\n      pivot(\"prediction\", (1 to 7)).\n      count().\n      na.fill(0.0).\n      orderBy(\"Cover_Type\")\n\n    confusionMatrix.show()\n",
      "user": "anonymous",
      "dateUpdated": "Mar 20, 2018 9:43:40 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "inputCols: Array[String] \u003d Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, S...assembler: org.apache.spark.ml.feature.VectorAssembler \u003d vecAssembler_5e0f2a3f851f\nassembledTrainData: org.apache.spark.sql.DataFrame \u003d [Elevation: int, Aspect: int ... 54 more fields]\n+-----------------------------------------------------------------------------------------------------+\n|featureVector                                                                                        |\n+-----------------------------------------------------------------------------------------------------+\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0])  |\n|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                 |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1879.0,28.0,19.0,30.0,12.0,95.0,209.0,196.0,117.0,778.0,1.0,1.0])   |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1888.0,33.0,22.0,150.0,46.0,108.0,209.0,185.0,103.0,735.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1889.0,28.0,22.0,150.0,23.0,120.0,205.0,185.0,108.0,759.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1889.0,353.0,30.0,95.0,39.0,67.0,153.0,172.0,146.0,600.0,1.0,1.0])  |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1896.0,337.0,12.0,30.0,6.0,175.0,195.0,224.0,168.0,732.0,1.0,1.0])  |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1899.0,355.0,22.0,153.0,43.0,124.0,178.0,195.0,151.0,819.0,1.0,1.0])|\n|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1901.0,311.0,9.0,30.0,2.0,190.0,195.0,234.0,179.0,726.0,1.0,1.0])   |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1903.0,5.0,13.0,42.0,4.0,201.0,203.0,214.0,148.0,708.0,1.0,1.0])    |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1903.0,67.0,16.0,108.0,36.0,120.0,234.0,207.0,100.0,969.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1905.0,19.0,27.0,134.0,58.0,120.0,188.0,171.0,108.0,636.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1905.0,33.0,27.0,90.0,46.0,150.0,204.0,171.0,89.0,725.0,1.0,1.0])   |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,16],[1905.0,77.0,21.0,90.0,38.0,120.0,241.0,196.0,75.0,1025.0,1.0,1.0])  |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1906.0,356.0,20.0,150.0,55.0,120.0,184.0,201.0,151.0,726.0,1.0,1.0])|\n|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1908.0,323.0,32.0,150.0,52.0,120.0,125.0,190.0,196.0,765.0,1.0,1.0])|\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1916.0,24.0,25.0,212.0,74.0,175.0,197.0,177.0,105.0,789.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1916.0,320.0,24.0,190.0,60.0,162.0,151.0,210.0,195.0,832.0,1.0,1.0])|\n|(54,[0,1,2,3,4,5,6,7,8,9,13,23],[1918.0,321.0,28.0,42.0,17.0,85.0,139.0,201.0,196.0,402.0,1.0,1.0])  |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1919.0,30.0,22.0,67.0,9.0,256.0,208.0,188.0,107.0,661.0,1.0,1.0])   |\n+-----------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\nclassifier: org.apache.spark.ml.classification.DecisionTreeClassifier \u003d dtc_8f2aed40cf03\nmodel: org.apache.spark.ml.classification.DecisionTreeClassificationModel \u003d DecisionTreeClassificationModel (uid\u003ddtc_8f2aed40cf03) of depth 5 with 63 nodes\nDecisionTreeClassificationModel (uid\u003ddtc_8f2aed40cf03) of depth 5 with 63 nodes\n  If (feature 0 \u003c\u003d 3050.0)\n   If (feature 0 \u003c\u003d 2557.0)\n    If (feature 10 \u003c\u003d 0.0)\n     If (feature 0 \u003c\u003d 2445.0)\n      If (feature 3 \u003c\u003d 0.0)\n       Predict: 4.0\n      Else (feature 3 \u003e 0.0)\n       Predict: 3.0\n     Else (feature 0 \u003e 2445.0)\n      If (feature 17 \u003c\u003d 0.0)\n       Predict: 2.0\n      Else (feature 17 \u003e 0.0)\n       Predict: 3.0\n    Else (feature 10 \u003e 0.0)\n     If (feature 9 \u003c\u003d 4601.0)\n      If (feature 22 \u003c\u003d 0.0)\n       Predict: 2.0\n      Else (feature 22 \u003e 0.0)\n       Predict: 2.0\n     Else (feature 9 \u003e 4601.0)\n      If (feature 5 \u003c\u003d 890.0)\n       Predict: 2.0\n      Else (feature 5 \u003e 890.0)\n       Predict: 2.0\n   Else (feature 0 \u003e 2557.0)\n    If (feature 0 \u003c\u003d 2958.0)\n     If (feature 15 \u003c\u003d 0.0)\n      If (feature 17 \u003c\u003d 0.0)\n       Predict: 2.0\n      Else (feature 17 \u003e 0.0)\n       Predict: 3.0\n     Else (feature 15 \u003e 0.0)\n      If (feature 9 \u003c\u003d 1421.0)\n       Predict: 3.0\n      Else (feature 9 \u003e 1421.0)\n       Predict: 3.0\n    Else (feature 0 \u003e 2958.0)\n     If (feature 3 \u003c\u003d 182.0)\n      If (feature 36 \u003c\u003d 0.0)\n       Predict: 2.0\n      Else (feature 36 \u003e 0.0)\n       Predict: 1.0\n     Else (feature 3 \u003e 182.0)\n      If (feature 7 \u003c\u003d 218.0)\n       Predict: 2.0\n      Else (feature 7 \u003e 218.0)\n       Predict: 2.0\n  Else (feature 0 \u003e 3050.0)\n   If (feature 0 \u003c\u003d 3309.0)\n    If (feature 7 \u003c\u003d 239.0)\n     If (feature 45 \u003c\u003d 0.0)\n      If (feature 42 \u003c\u003d 0.0)\n       Predict: 1.0\n      Else (feature 42 \u003e 0.0)\n       Predict: 1.0\n     Else (feature 45 \u003e 0.0)\n      If (feature 5 \u003c\u003d 4146.0)\n       Predict: 1.0\n      Else (feature 5 \u003e 4146.0)\n       Predict: 1.0\n    Else (feature 7 \u003e 239.0)\n     If (feature 3 \u003c\u003d 331.0)\n      If (feature 0 \u003c\u003d 3203.0)\n       Predict: 1.0\n      Else (feature 0 \u003e 3203.0)\n       Predict: 1.0\n     Else (feature 3 \u003e 331.0)\n      If (feature 0 \u003c\u003d 3203.0)\n       Predict: 2.0\n      Else (feature 0 \u003e 3203.0)\n       Predict: 2.0\n   Else (feature 0 \u003e 3309.0)\n    If (feature 12 \u003c\u003d 0.0)\n     If (feature 3 \u003c\u003d 285.0)\n      If (feature 6 \u003c\u003d 206.0)\n       Predict: 1.0\n      Else (feature 6 \u003e 206.0)\n       Predict: 7.0\n     Else (feature 3 \u003e 285.0)\n      If (feature 11 \u003c\u003d 0.0)\n       Predict: 1.0\n      Else (feature 11 \u003e 0.0)\n       Predict: 1.0\n    Else (feature 12 \u003e 0.0)\n     If (feature 45 \u003c\u003d 0.0)\n      If (feature 0 \u003c\u003d 3369.0)\n       Predict: 7.0\n      Else (feature 0 \u003e 3369.0)\n       Predict: 7.0\n     Else (feature 45 \u003e 0.0)\n      If (feature 5 \u003c\u003d 997.0)\n       Predict: 7.0\n      Else (feature 5 \u003e 997.0)\n       Predict: 1.0\n\n(0.7796919815211103,Elevation)\n(0.0398128237749954,Horizontal_Distance_To_Hydrology)\n(0.03156107154830829,Hillshade_Noon)\n(0.030198698429345045,Soil_Type_3)\n(0.0282780732216438,Wilderness_Area_0)\n(0.026920786045451794,Soil_Type_31)\n(0.023965641197494086,Soil_Type_1)\n(0.011002916912030062,Wilderness_Area_2)\n(0.009654840511466256,Soil_Type_28)\n(0.0058962988381604554,Horizontal_Distance_To_Roadways)\n(0.005292807192707336,Soil_Type_22)\n(0.0031396067779140015,Wilderness_Area_1)\n(0.00255369485868422,Hillshade_9am)\n(0.0017475453048820069,Horizontal_Distance_To_Fire_Points)\n(2.832138658070966E-4,Soil_Type_8)\n(0.0,Wilderness_Area_3)\n(0.0,Vertical_Distance_To_Hydrology)\n(0.0,Soil_Type_9)\n(0.0,Soil_Type_7)\n(0.0,Soil_Type_6)\n(0.0,Soil_Type_5)\n(0.0,Soil_Type_4)\n(0.0,Soil_Type_39)\n(0.0,Soil_Type_38)\n(0.0,Soil_Type_37)\n(0.0,Soil_Type_36)\n(0.0,Soil_Type_35)\n(0.0,Soil_Type_34)\n(0.0,Soil_Type_33)\n(0.0,Soil_Type_32)\n(0.0,Soil_Type_30)\n(0.0,Soil_Type_29)\n(0.0,Soil_Type_27)\n(0.0,Soil_Type_26)\n(0.0,Soil_Type_25)\n(0.0,Soil_Type_24)\n(0.0,Soil_Type_23)\n(0.0,Soil_Type_21)\n(0.0,Soil_Type_20)\n(0.0,Soil_Type_2)\n(0.0,Soil_Type_19)\n(0.0,Soil_Type_18)\n(0.0,Soil_Type_17)\n(0.0,Soil_Type_16)\n(0.0,Soil_Type_15)\n(0.0,Soil_Type_14)\n(0.0,Soil_Type_13)\n(0.0,Soil_Type_12)\n(0.0,Soil_Type_11)\n(0.0,Soil_Type_10)\n(0.0,Soil_Type_0)\n(0.0,Slope)\n(0.0,Hillshade_3pm)\n(0.0,Aspect)\npredictions: org.apache.spark.sql.DataFrame \u003d [Elevation: int, Aspect: int ... 57 more fields]\n+----------+----------+------------------------------------------------------------------------------------------------+\n|Cover_Type|prediction|probability                                                                                     |\n+----------+----------+------------------------------------------------------------------------------------------------+\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |4.0       |[0.0,0.0,0.04187082405345212,0.2846325167037862,0.44097995545657015,0.0,0.23251670378619155,0.0]|\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|3.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|3.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|3.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n|6.0       |3.0       |[0.0,0.0,0.0316138592900995,0.636581751464258,0.05130195469621057,0.0,0.2805024345494319,0.0]   |\n+----------+----------+------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\nevaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \u003d mcEval_716186fd4cb0\naccuracy: Double \u003d 0.7002025193483429\nf1: Double \u003d 0.6826375331450174\n0.7002025193483429\n0.6826375331450174\npredictionRDD: org.apache.spark.rdd.RDD[(Double, Double)] \u003d MapPartitionsRDD[79] at rdd at \u003cconsole\u003e:57\nmulticlassMetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics \u003d org.apache.spark.mllib.evaluation.MulticlassMetrics@2486dcca\n128315.0  56428.0   161.0    0.0    0.0  0.0  5709.0   \n49194.0   200664.0  4268.0   94.0   0.0  0.0  849.0    \n0.0       5983.0    25665.0  639.0  0.0  0.0  0.0      \n0.0       21.0      1454.0   990.0  0.0  0.0  0.0      \n2.0       7815.0    734.0    0.0    0.0  0.0  0.0      \n0.0       6362.0    8668.0   522.0  0.0  0.0  0.0      \n7642.0    169.0     54.0     0.0    0.0  0.0  10511.0  \nconfusionMatrix: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] \u003d [Cover_Type: double, 1: bigint ... 6 more fields]\n+----------+------+------+-----+---+---+---+-----+\n|Cover_Type|     1|     2|    3|  4|  5|  6|    7|\n+----------+------+------+-----+---+---+---+-----+\n|       1.0|128315| 56428|  161|  0|  0|  0| 5709|\n|       2.0| 49194|200664| 4268| 94|  0|  0|  849|\n|       3.0|     0|  5983|25665|639|  0|  0|    0|\n|       4.0|     0|    21| 1454|990|  0|  0|    0|\n|       5.0|     2|  7815|  734|  0|  0|  0|    0|\n|       6.0|     0|  6362| 8668|522|  0|  0|    0|\n|       7.0|  7642|   169|   54|  0|  0|  0|10511|\n+----------+------+------+-----+---+---+---+-----+\n\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1521210044351_-1760296633",
      "id": "20180316-232044_11338969",
      "dateCreated": "Mar 16, 2018 11:20:44 PM",
      "dateStarted": "Mar 20, 2018 9:44:04 PM",
      "dateFinished": "Mar 20, 2018 9:45:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "  def classProbabilities(data: DataFrame): Array[Double] \u003d {\n    val total \u003d data.count()\n    data.groupBy(\"Cover_Type\").count().\n      orderBy(\"Cover_Type\").\n      select(\"count\").as[Double].\n      map(_ / total).\n      collect()\n  }\n\n  def randomClassifier(trainData: DataFrame, testData: DataFrame): Unit \u003d {\n    val trainPriorProbabilities \u003d classProbabilities(trainData)\n    val testPriorProbabilities \u003d classProbabilities(testData)\n    val accuracy \u003d trainPriorProbabilities.zip(testPriorProbabilities).map {\n      case (trainProb, cvProb) \u003d\u003e trainProb * cvProb\n    }.sum\n    println(accuracy)\n  }\n\n  def evaluate(trainData: DataFrame, testData: DataFrame): Unit \u003d {\n\n    val inputCols \u003d trainData.columns.filter(_ !\u003d \"Cover_Type\")\n    val assembler \u003d new VectorAssembler().\n      setInputCols(inputCols).\n      setOutputCol(\"featureVector\")\n\n    val classifier \u003d new DecisionTreeClassifier().\n      setSeed(Random.nextLong()).\n      setLabelCol(\"Cover_Type\").\n      setFeaturesCol(\"featureVector\").\n      setPredictionCol(\"prediction\")\n\n    val pipeline \u003d new Pipeline().setStages(Array(assembler, classifier))\n\n    val paramGrid \u003d new ParamGridBuilder().\n      addGrid(classifier.impurity, Seq(\"gini\", \"entropy\")).\n      addGrid(classifier.maxDepth, Seq(1, 20)).\n      addGrid(classifier.maxBins, Seq(40, 300)).\n      addGrid(classifier.minInfoGain, Seq(0.0, 0.05)).\n      build()\n\n    val multiclassEval \u003d new MulticlassClassificationEvaluator().\n      setLabelCol(\"Cover_Type\").\n      setPredictionCol(\"prediction\").\n      setMetricName(\"accuracy\")\n\n    val validator \u003d new TrainValidationSplit().\n      setSeed(Random.nextLong()).\n      setEstimator(pipeline).\n      setEvaluator(multiclassEval).\n      setEstimatorParamMaps(paramGrid).\n      setTrainRatio(0.9)\n\n    val validatorModel \u003d validator.fit(trainData)\n\n    val paramsAndMetrics \u003d validatorModel.validationMetrics.\n      zip(validatorModel.getEstimatorParamMaps).sortBy(-_._1)\n\n    paramsAndMetrics.foreach { case (metric, params) \u003d\u003e\n        println(metric)\n        println(params)\n        println()\n    }\n\n    val bestModel \u003d validatorModel.bestModel\n\n    println(bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)\n\n    println(validatorModel.validationMetrics.max)\n\n    val testAccuracy \u003d multiclassEval.evaluate(bestModel.transform(testData))\n    println(testAccuracy)\n\n    val trainAccuracy \u003d multiclassEval.evaluate(bestModel.transform(trainData))\n    println(trainAccuracy)\n  }\n\n  def unencodeOneHot(data: DataFrame): DataFrame \u003d {\n    val wildernessCols \u003d (0 until 4).map(i \u003d\u003e s\"Wilderness_Area_$i\").toArray\n\n    val wildernessAssembler \u003d new VectorAssembler().\n      setInputCols(wildernessCols).\n      setOutputCol(\"wilderness\")\n\n    val unhotUDF \u003d udf((vec: Vector) \u003d\u003e vec.toArray.indexOf(1.0).toDouble)\n\n    val withWilderness \u003d wildernessAssembler.transform(data).\n      drop(wildernessCols:_*).\n      withColumn(\"wilderness\", unhotUDF($\"wilderness\"))\n\n    val soilCols \u003d (0 until 40).map(i \u003d\u003e s\"Soil_Type_$i\").toArray\n\n    val soilAssembler \u003d new VectorAssembler().\n      setInputCols(soilCols).\n      setOutputCol(\"soil\")\n\n    soilAssembler.transform(withWilderness).\n      drop(soilCols:_*).\n      withColumn(\"soil\", unhotUDF($\"soil\"))\n  }\n\n  def evaluateCategorical(trainData: DataFrame, testData: DataFrame): Unit \u003d {\n    val unencTrainData \u003d unencodeOneHot(trainData)\n    val unencTestData \u003d unencodeOneHot(testData)\n\n    val inputCols \u003d unencTrainData.columns.filter(_ !\u003d \"Cover_Type\")\n    val assembler \u003d new VectorAssembler().\n      setInputCols(inputCols).\n      setOutputCol(\"featureVector\")\n\n    val indexer \u003d new VectorIndexer().\n      setMaxCategories(40).\n      setInputCol(\"featureVector\").\n      setOutputCol(\"indexedVector\")\n\n    val classifier \u003d new DecisionTreeClassifier().\n      setSeed(Random.nextLong()).\n      setLabelCol(\"Cover_Type\").\n      setFeaturesCol(\"indexedVector\").\n      setPredictionCol(\"prediction\")\n\n    val pipeline \u003d new Pipeline().setStages(Array(assembler, indexer, classifier))\n\n    val paramGrid \u003d new ParamGridBuilder().\n      addGrid(classifier.impurity, Seq(\"gini\", \"entropy\")).\n      addGrid(classifier.maxDepth, Seq(1, 20)).\n      addGrid(classifier.maxBins, Seq(40, 300)).\n      addGrid(classifier.minInfoGain, Seq(0.0, 0.05)).\n      build()\n\n    val multiclassEval \u003d new MulticlassClassificationEvaluator().\n      setLabelCol(\"Cover_Type\").\n      setPredictionCol(\"prediction\").\n      setMetricName(\"accuracy\")\n\n    val validator \u003d new TrainValidationSplit().\n      setSeed(Random.nextLong()).\n      setEstimator(pipeline).\n      setEvaluator(multiclassEval).\n      setEstimatorParamMaps(paramGrid).\n      setTrainRatio(0.9)\n\n    val validatorModel \u003d validator.fit(unencTrainData)\n\n    val bestModel \u003d validatorModel.bestModel\n\n    println(bestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap)\n\n    val testAccuracy \u003d multiclassEval.evaluate(bestModel.transform(unencTestData))\n    println(testAccuracy)\n  }\n\n  def evaluateForest(trainData: DataFrame, testData: DataFrame): Unit \u003d {\n    val unencTrainData \u003d unencodeOneHot(trainData)\n    val unencTestData \u003d unencodeOneHot(testData)\n\n    val inputCols \u003d unencTrainData.columns.filter(_ !\u003d \"Cover_Type\")\n    val assembler \u003d new VectorAssembler().\n      setInputCols(inputCols).\n      setOutputCol(\"featureVector\")\n\n    val indexer \u003d new VectorIndexer().\n      setMaxCategories(40).\n      setInputCol(\"featureVector\").\n      setOutputCol(\"indexedVector\")\n\n    val classifier \u003d new RandomForestClassifier().\n      setSeed(Random.nextLong()).\n      setLabelCol(\"Cover_Type\").\n      setFeaturesCol(\"indexedVector\").\n      setPredictionCol(\"prediction\").\n      setImpurity(\"entropy\").\n      setMaxDepth(20).\n      setMaxBins(300)\n\n    val pipeline \u003d new Pipeline().setStages(Array(assembler, indexer, classifier))\n\n    val paramGrid \u003d new ParamGridBuilder().\n      addGrid(classifier.minInfoGain, Seq(0.0, 0.05)).\n      addGrid(classifier.numTrees, Seq(1, 10)).\n      build()\n\n    val multiclassEval \u003d new MulticlassClassificationEvaluator().\n      setLabelCol(\"Cover_Type\").\n      setPredictionCol(\"prediction\").\n      setMetricName(\"accuracy\")\n\n    val validator \u003d new TrainValidationSplit().\n      setSeed(Random.nextLong()).\n      setEstimator(pipeline).\n      setEvaluator(multiclassEval).\n      setEstimatorParamMaps(paramGrid).\n      setTrainRatio(0.9)\n\n    val validatorModel \u003d validator.fit(unencTrainData)\n\n    val bestModel \u003d validatorModel.bestModel\n\n    val forestModel \u003d bestModel.asInstanceOf[PipelineModel].\n      stages.last.asInstanceOf[RandomForestClassificationModel]\n\n    println(forestModel.extractParamMap)\n    println(forestModel.getNumTrees)\n    forestModel.featureImportances.toArray.zip(inputCols).\n      sorted.reverse.foreach(println)\n\n    val testAccuracy \u003d multiclassEval.evaluate(bestModel.transform(unencTestData))\n    println(testAccuracy)\n\n    bestModel.transform(unencTestData.drop(\"Cover_Type\")).select(\"prediction\").show()\n  }\n",
      "user": "anonymous",
      "dateUpdated": "Mar 20, 2018 9:43:40 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "classProbabilities: (data: org.apache.spark.sql.DataFrame)Array[Double]\nrandomClassifier: (trainData: org.apache.spark.sql.DataFrame, testData: org.apache.spark.sql.DataFrame)Unit\nevaluate: (trainData: org.apache.spark.sql.DataFrame, testData: org.apache.spark.sql.DataFrame)Unit\nunencodeOneHot: (data: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\nevaluateCategorical: (trainData: org.apache.spark.sql.DataFrame, testData: org.apache.spark.sql.DataFrame)Unit\nevaluateForest: (trainData: org.apache.spark.sql.DataFrame, testData: org.apache.spark.sql.DataFrame)Unit\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1521210493204_2070503146",
      "id": "20180316-232813_1469547240",
      "dateCreated": "Mar 16, 2018 11:28:13 PM",
      "dateStarted": "Mar 20, 2018 9:44:16 PM",
      "dateFinished": "Mar 20, 2018 9:45:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//evaluateCategorical(trainData,testData)",
      "user": "anonymous",
      "dateUpdated": "Mar 20, 2018 9:43:40 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "jobName": "paragraph_1521209763646_-1698166525",
      "id": "20180316-231603_762444752",
      "dateCreated": "Mar 16, 2018 11:16:03 PM",
      "dateStarted": "Mar 20, 2018 9:45:44 PM",
      "dateFinished": "Mar 20, 2018 9:45:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "evaluateForest(trainData,testData)",
      "user": "anonymous",
      "dateUpdated": "Mar 20, 2018 9:43:40 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 71.0 failed 1 times, most recent failure: Lost task 3.0 in stage 71.0 (TID 648, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.\u003cinit\u003e(DTStatsAggregator.scala:77)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$10$$anonfun$11.apply(RandomForest.scala:541)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$10$$anonfun$11.apply(RandomForest.scala:537)\n\tat scala.Array$.tabulate(Array.scala:331)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$10.apply(RandomForest.scala:537)\n\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$10.apply(RandomForest.scala:534)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:748)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:747)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:747)\n  at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:563)\n  at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:198)\n  at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:137)\n  at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:45)\n  at org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n  at org.apache.spark.ml.Predictor.fit(Predictor.scala:72)\n  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)\n  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:893)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n  at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)\n  at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)\n  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)\n  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:96)\n  at org.apache.spark.ml.Estimator.fit(Estimator.scala:61)\n  at org.apache.spark.ml.Estimator$$anonfun$fit$1.apply(Estimator.scala:82)\n  at org.apache.spark.ml.Estimator$$anonfun$fit$1.apply(Estimator.scala:82)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n  at org.apache.spark.ml.Estimator.fit(Estimator.scala:82)\n  at org.apache.spark.ml.tuning.TrainValidationSplit.fit(TrainValidationSplit.scala:107)\n  at evaluateForest(\u003cconsole\u003e:100)\n  ... 50 elided\nCaused by: java.lang.OutOfMemoryError: Java heap space\n  at org.apache.spark.ml.tree.impl.DTStatsAggregator.\u003cinit\u003e(DTStatsAggregator.scala:77)\n  at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$10$$anonfun$11.apply(RandomForest.scala:541)\n  at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$10$$anonfun$11.apply(RandomForest.scala:537)\n  at scala.Array$.tabulate(Array.scala:331)\n  at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$10.apply(RandomForest.scala:537)\n  at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$10.apply(RandomForest.scala:534)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n  ... 3 more\n"
          }
        ]
      },
      "apps": [],
      "jobName": "paragraph_1521291411660_457193541",
      "id": "20180317-215651_2013411920",
      "dateCreated": "Mar 17, 2018 9:56:51 PM",
      "dateStarted": "Mar 20, 2018 9:45:47 PM",
      "dateFinished": "Mar 20, 2018 9:46:30 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "user": "anonymous",
      "dateUpdated": "Mar 20, 2018 9:43:40 PM",
      "config": {
        "colWidth": 12.0,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala"
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "jobName": "paragraph_1521294591345_-433351855",
      "id": "20180317-224951_161670947",
      "dateCreated": "Mar 17, 2018 10:49:51 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Forest",
  "id": "2DAY8SE3N",
  "angularObjects": {
    "2DDH2WQSB:shared_process": [],
    "2DBBSXRRW:shared_process": [],
    "2DAPSSHCT:shared_process": [],
    "2DCR57YCT:shared_process": [],
    "2DAXDMBAJ:shared_process": [],
    "2DBRX1AQT:shared_process": [],
    "2DDRKEEGG:shared_process": [],
    "2DBUPDJFC:shared_process": [],
    "2DCE5XZDR:shared_process": [],
    "2DBVYT3CU:shared_process": [],
    "2DD3TM72G:shared_process": [],
    "2DAYVGWBM:shared_process": [],
    "2DAVW3PTU:shared_process": [],
    "2DDVJ4SYP:shared_process": [],
    "2DA9JSGRJ:shared_process": [],
    "2DBFUDA3G:shared_process": [],
    "2DDX9CBD7:shared_process": [],
    "2DCGM69PY:shared_process": []
  },
  "config": {},
  "info": {}
}